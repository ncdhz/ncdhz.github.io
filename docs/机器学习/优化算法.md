# 优化算法

## 梯度下降 (Gradient Descent)

爬山的时候最陡的地方能够让你更快的到达山顶（直角三角形的直角边长度小于斜边）。而梯度下降的核心思想是：每次在当前梯度方向（最陡峭的方向）前进一步，来逼近函数的最小值（山顶）。

$$
\theta_i = \theta_{i-1} - \lambda g(\theta_{i-1})
$$

+ $g(\theta_{i-1})$ 所有点构成的损失函数的导数(也就是梯度)
+ $\lambda$ 学习率，由于梯度只是一个方向，在这个方向走多远就看这个学习率了

## 随机梯度下降（Stochastic Gradient Descent）

 由于梯度下降法每次计算时包含整个数据集，会影响计算的速度，而随机梯度下降(SGD)，在每一步的梯度计算上只随机选取训练集中的一个样本。

## 小批量梯度下降法 (Mini-batch-Gradient-Descent)

把随机梯度下降中的一个样本改成了，一小批样本。小批量梯度下降会比随机梯度更靠近最小值。

## 动量梯度下降法 （Gradient descent with Momentum）

动量梯度下降法算法就是让每一次参数更新都不仅仅取决于当前位置的梯度，还要取决于上一次上上一次的梯度，而且离当前位置越远的梯度对参数更新的影响越小。
$$
\begin{split}
d_i &= \beta d_{i-1} - \lambda g(\theta_{i-1}) \\
\theta_i &= \theta_{i-1} +  d_i
\end{split}
$$

## 加速梯度 ( Nesterov Accelerated Gradient)

既然下一次一定会更新 $\beta d_{i-1}$ 那么求梯度的时候就可以用提前位置的梯度 $g(\theta_{i-1} + \beta d_{i-1})$
$$
\begin{split}
d_i &= \beta_{i-1} - \lambda g(\theta_{i-1} + \beta d_{i-1}) \\
\theta_i &= \theta_{i-1} + d_i
\end{split}
$$

## 自适应梯度法

## Adagrad

一般学习率都希望开始时大点然后后面越来越小，这样可以快速的到达最低点也能避免越过最低点（因为学习率决定了每一次向前走的距离，前面离的远可以快点走，后面离的太近了就要慢点走），而 Adagrad 就能很好的做到这点。
$$
\begin{split}
c_i &= c_(i-1) + g^2(\theta_{i-1}) \\
\theta_i &= \theta_{i-1} - \frac{\lambda}{\sqrt {c_i + \epsilon}} g(\theta_{i-1})
\end{split}
$$

+ $c_i$ 用于保存各位置梯度的平方
+ $\epsilon$ 一般取值为$10^{-4}$～$10^{-8}$，为了防止分母取零
+ Adagrad 不需要手动调节学习率 $\lambda$
+ 学习率 $\frac{\lambda}{\sqrt {c_i + \epsilon}}$ 他会随着 $c_i$ 的变大而逐步变小
+ 缺点是：学习率会一直变小最后可能会导致参数无法更新

## Rmsprop

$$
\begin{split}
c_i &= \gamma c_(i-1) + (1-\gamma)g^2(\theta_{i-1}) \\
\theta_i &= \theta_{i-1} - \frac{\lambda}{\sqrt {c_i + \epsilon}} g(\theta_{i-1})
\end{split}
$$
可以看出Rmsprop与Adagrad类似，只不过cache的计算略微复杂一些，使用到了一个衰减因子 $\gamma$

+ 衰减因子解决了Adagrad学习率迅速减小的问题
+ 衰减因子$\gamma$通常取值为$[0.9，0.99，0.999]$

## Adadelta

$$
\begin{split}
c_i &= \gamma c_{i-1} + (1 - \gamma) g^2(\theta_{i-1}) \\
d_i &= - \frac{\sqrt{\Delta_{i-1}+\epsilon}}{\sqrt{c_i+\epsilon}}{g(\theta_{i-1})} \\
\theta_i &= \theta_{i-1} + d_i \\
\Delta_i &= \gamma \Delta_{i-1} + (1-\gamma)d_i^2
\end{split}
$$
Adadelta与Rmsprop类似，但是连初始的学习速率 $\lambda$ 都不用设置

+ 学习率分子取决于 $\gamma \Delta_{i-1} + (1-\gamma)d_i^2$
+ $\sqrt{\Delta_{i-1}+\epsilon}$ 中的 $\epsilon$ 是为了避免分子为0也就是确定最开始的学习率

## Adam

$$
\begin{split}
m_i &= \beta_1 m_{i-1} + (1-\beta_1)g(\theta_{i-1}) \\
v_i &= \beta_2 v_{i-1} + (1-\beta_2)g^2(\theta_{i-1}) \\
\hat{m_i} &= \frac{m_i}{1-\beta_i^t} \\
\hat{v_i} &= \frac{v_i}{1-\beta^t_2} \\
\theta_i &= \theta_{i-1} - \frac{\lambda}{\sqrt{\hat{v_i} + \epsilon}}{\hat{m_i}}
\end{split}
$$

与Rmsprop类似，Adam除了利用一个衰减因子$\beta_2$计算cache以外，还类似Momentum，利用了上一次的参数更新方向。在参数更新的最初几步中，由于$m_i$与$v_i$是初始化为0的，为了防止最初几步的更新向0偏差，Adam利用$\beta_i$的$t$次幂来修正这种偏差（$t$每次更新加1）。其中，通常$\beta_1$取值为0.9，$\beta_2$取值为0.999，$\epsilon$取值为$10^-8$。

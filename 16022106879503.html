<!DOCTYPE html>
<html lang="zh">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>
      
    线性回归 - NCDHZ
    
    </title>
    

    
    
    <link href="atom.xml" rel="alternate" title="NCDHZ" type="application/atom+xml">
    <link rel="stylesheet" href="asset/css/style.min.css">
    <link rel="stylesheet" href="asset/css/doc.css">
    <script src="asset/app.js"></script>
</head>
  <body>
    <section class="hero">
      <div class="hero-head">
          <nav class="navbar" role="navigation" aria-label="main navigation">
              <div class="container">
              <div class="navbar-brand">
                
                <a target="_self" class="navbar-item " href="https://www.majunlong.top">个人网站</a>
                
                <a target="_self" class="navbar-item " href="index.html">Home</a>
                
                <a target="_self" class="navbar-item " href="archives.html">Archives</a>
                

                <a role="button" id="navbarSNSRssSwitchBtn" class="navbar-burger burger" aria-label="menu" aria-expanded="false" data-target="navbarSNSRssButtons">
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                  <span aria-hidden="true"></span>
                </a>
              </div>
            
              <div id="navbarSNSRssButtons" class="navbar-menu">
                <div class="navbar-start">
                  
                </div>
            
                <div class="navbar-end">
                  <div class="navbar-item">
                    <!--buttons start-->
                    <div class="buttons">
                      
                        
                        
                        
                        <a href="https://github.com/ncdhz" target="_blank" title="github">
                            <span class="icon is-large has-text-grey-darker">
                               <svg class="svg-inline--fa fa-github fa-w-16 fa-lg" aria-hidden="true" focusable="false" data-prefix="fab" data-icon="github" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 496 512" data-fa-i2svg=""><path fill="currentColor" d="M165.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6zm-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3zm44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9zM244.8 8C106.1 8 0 113.3 0 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C428.2 457.8 496 362.9 496 252 496 113.3 383.5 8 244.8 8zM97.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1zm-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7zm32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1zm-11.4-14.7c-1.6 1-1.6 3.6 0 5.9 1.6 2.3 4.3 3.3 5.6 2.3 1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2z"></path></svg><!-- <i class="fab fa-github fa-lg"></i> -->
                            </span>
                          </a>
                        
                        
                      
                      <a href="atom.xml" target="_blank" title="RSS">
                          <span class="icon is-large has-text-black-bis">
                              <svg class="svg-inline--fa fa-rss fa-w-14 fa-lg" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="rss" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" data-fa-i2svg=""><path fill="currentColor" d="M128.081 415.959c0 35.369-28.672 64.041-64.041 64.041S0 451.328 0 415.959s28.672-64.041 64.041-64.041 64.04 28.673 64.04 64.041zm175.66 47.25c-8.354-154.6-132.185-278.587-286.95-286.95C7.656 175.765 0 183.105 0 192.253v48.069c0 8.415 6.49 15.472 14.887 16.018 111.832 7.284 201.473 96.702 208.772 208.772.547 8.397 7.604 14.887 16.018 14.887h48.069c9.149.001 16.489-7.655 15.995-16.79zm144.249.288C439.596 229.677 251.465 40.445 16.503 32.01 7.473 31.686 0 38.981 0 48.016v48.068c0 8.625 6.835 15.645 15.453 15.999 191.179 7.839 344.627 161.316 352.465 352.465.353 8.618 7.373 15.453 15.999 15.453h48.068c9.034-.001 16.329-7.474 16.005-16.504z"></path></svg><!-- <i class="fas fa-rss fa-lg"></i> -->
                          </span>
                      </a>
                    </div>
                    <!--buttons end-->

                  </div>
                </div>
                </div>
              </div>
            </nav>
      </div>

 <div class="hero-body ct-body"></div>
      
    </section>
    <section class="ct-body">
      <div class="container">
          <div class="columns is-variable bd-klmn-columns is-4 is-centered">
              <div class="column is-four-fifths">
                  <div class="post-body single-content">
                    
                    <h1 class="title">
                            线性回归   
                      </h1>
                     
                    
                      <div class="media">
                            
                            <figure class="media-left">
                              <p class="image is-48x48">
                                
                                  <img class="is-rounded" src="">
                                
                              </p>
                            </figure>
                            
                            <div class="media-content">
                              <div class="content">
                                <p>
                                 <span class="date">2020/10/09</span>
                                  <span class="tran-posted-in">posted in</span>&nbsp; 
                                  
                                      <span class="posted-in"><a href='%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>机器学习</a></span>
                                  
                                      <span class="posted-in"><a href='Python.html'>Python</a></span>
                                         
                                  

                                   
                                      
                                  <br />
                                  <span class="tran-tags">Tags:</span>&nbsp;
                                  
                                    <a class="tag is-link is-light" href='tag_%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0.html'>#机器学习</a>
                                  
                                    <a class="tag is-link is-light" href='tag_Python.html'>#Python</a>
                                     

                                </p>
                              </div>
                            </div>
                         
                    </div>
                </div>
                  <article class="markdown-body single-content">
                    <ol>
<li><p>这个项目用了下面的库</p>
<pre><code class="language-text">numpy
matplotlib
</code></pre></li>
<li><p>这个项目的数据格式<a href="https://raw.githubusercontent.com/ncdhz/deep-learning-study-code/master/linear-regression/data.csv">点击链接下载数据</a></p>
<pre><code class="language-text">这个项目的数据有两项，我们可以理解成x坐标和y坐标 
如下图<br/>
x: 32.502345269453031<br/>
y: 31.70700584656992
</code></pre>
<p><img src="media/16022106879503/16022114075502.png" alt=""/></p>
<p><img src="media/16022106879503/16022110880086.jpg" alt=""/></p></li>
<li><p>引入项目所需要的库</p>
<pre><code class="language-text"># 用于处理数据
import numpy as np<br/>
# 用于画图<br/>
import matplotlib.pyplot as plt
</code></pre></li>
<li><p>损失函数<br/>
\[loss=\sum_{i=1}^{n}{(w*x_i+b-y_i)^2}\]</p>
<ul>
<li>为什么这样定义损失函数</li>
</ul>
<p>观察下面的图，我们画了一条直线准备穿过这两个点。显然这条直线没有达到预期，其中存在一定的误差。那该怎么表示这个误差呢，我们可以取一个  x 然后计算出 y 值，对比真实的 y 值也就是 \[w*x_i+b-y_i\] 这样我们就可以得到计算出来的 y 值和真实值之间的误差。但是这样会出现一个问题----误差会出现正负两种情况。所以就加上了绝对值 \[(w*x_i+b-y_i)^2\] 然后再把每个点的误差相加就形成了损失函数 \[\sum_{i=1}^{n}{(w*x_i+b-y_i)^2}\]</p>
<p><img src="media/16022106879503/16022204166202.png" alt=""/></p>
<ul>
<li>根据上面的损失函数编写代码</li>
</ul>
<pre><code class="language-text">def compute_error_for_line_given_points(b, w, points):
    # 用于保存累加和<br/>
    totalError = 0<br/>
    # 这个for循环就相当于上面的累加符号<br/>
    for i in range(0, len(points)):<br/>
        # 取出二维数组中的第i个数组的第一个数据<br/>
        x = points[i, 0]<br/>
        # 取出二维数组中的第i个数组的第二个数据<br/>
        y = points[i, 1]<br/>
        # ((w * x + b) - y) ** 2 就是上面公式单个损失值<br/>
        totalError += ((w * x + b) - y) ** 2<br/>
    # 返回损失函数一般会除元素数量<br/>
    return totalError / float(len(points))
</code></pre></li>
<li><p>求解最小化<code>w</code> 和 <code>b</code></p>
<ul>
<li>目标优化式</li>
</ul>
<p>\[(w^*, b^*) = argmin\sum_{i=1}^{n}{(w*x_i+b-y_i)^2}\]</p>
<ul>
<li><p>求解的两种方法</p>
<ul>
<li>最小二乘法<br/>
我们先将损失函数loss对w求导和对w求导<br/>
\[<br/>
\begin{split}<br/>
\frac{\partial{loss}}{\partial{w}}&amp;={2\sum_{i=1}^{n}{(w*x_i+b-y_i)*x_i}} \\<br/>
&amp;=2({w\sum_{i=1}^{n}{x_i^2}-\sum_{i=1}^{n}{(y_i-b)*x_i}})<br/>
\end{split}<br/>
\]<br/>
\[<br/>
\begin{split}<br/>
\frac{\partial{loss}}{\partial{b}}&amp;={\sum_{i=1}^{n}2(w*x_i+b-y_i)} \\<br/>
&amp;= 2(n*b-\sum_{i=1}^{n}{(y_i-w*x_i)})<br/>
\end{split}<br/>
\]<br/>
让上两式为0可以得到 w 和 b 最优解的闭式(closed-form)解</li>
</ul>
<p>\[<br/>
\begin{eqnarray*}<br/>
w &amp;=&amp; \frac{\sum_{i=1}^{n}{y_i(x_i-\overline{x})}}{\sum_{i=1}^{n}{x_i^2}-{\frac{1}{n}{(\sum_{i=1}^{n}{x_i})^2}}} \\<br/>
\overline{x} &amp;=&amp;{\frac{1}{n}{\sum_{i=1}^{n}{x_i}}} \\<br/>
b &amp;=&amp; \frac{1}{n}\sum_{i=1}^{n}{(y_i-w*x_i)}<br/>
\end{eqnarray*}<br/>
\]</p>
<ul>
<li>最小二乘法代码</li>
</ul>
<pre><code class="language-text"># points 数据
def least_square_method(points):<br/>
    # x 的平均值<br/>
    x_average = 0<br/>
    # 数据的长度<br/>
    points_len = len(points)<br/>
    # 数据的长度浮点型<br/>
    N = float(points_len)<br/>
    # 累加 x <br/>
    for i in range(0, points_len):<br/>
        # points[i, 0] 数据 x<br/>
        x_average += points[i, 0]<br/>
    # w 式子中的3个子式 w_1 分母中左边那个 w_2 分母中右边那个 w_3 分子<br/>
    w_1 = w_2 = w_3 = 0<br/>
    # x_average 中存放的是 x 累加的和还没有平均<br/>
    # x 的平方除以数据长度<br/>
    w_2 = x_average ** 2 / N<br/>
    # 求出平均数<br/>
    x_average /= N<br/>
    # 求 w_1 和 w_3<br/>
    for i in range(0, points_len):<br/>
        w_1 += points[i, 0] ** 2<br/>
        w_3 += points[i, 1] * (points[i, 0] - x_average)<br/>
    # 计算 w<br/>
    w = w_3 / (w_1 - w_2)<br/>
    # 计算 b<br/>
    b = 0<br/>
    for i in range(0, points_len):<br/>
        b += (points[i, 1] - w * points[i, 0])<br/>
    b /= N<br/>
    return [b, w]
</code></pre>
<ul>
<li>通过最小二乘法求出来的结果</li>
</ul>
<pre><code class="language-text">def run_least_square_method():
    # 获取文件中的数据<br/>
    points = np.genfromtxt(&#39;data.csv&#39;, delimiter=&#39;,&#39;)<br/>
    # 求出直线的 w 和 b 值<br/>
    b, w = least_square_method(points)<br/>
    # 画图 x 点随机生成<br/>
    x = np.linspace(1, 100, 100)<br/>
    # 根据 x w b 求出 y<br/>
    y = x * w + b<br/>
    # 画数据点<br/>
    plt.scatter(points[:, 0], points[:, 1])<br/>
    # 划线<br/>
    plt.plot(x, y)<br/>
    plt.show()<br/>
    # 通过最小二乘法求出来的误差<br/>
    error = compute_error_for_line_given_points(b, w, points)<br/>
    print(&#39;error = {0}&#39;.format(error))
</code></pre>
<p><img src="media/16022106879503/16022587953466.png" alt=""/></p></li>
<li><p>梯度下降法<br/>
这个项目的损失函数<br/>
<img src="media/16022106879503/16022552932524.png" alt=""/><br/>
梯度下降的原理：损失函数可以说是错误率，这个值越小越好，显然只有到达这个图最低点时错误率就是最小的。假设这个曲面中间有一点，那么他用什么办法可以到达最低点呢（也就是他应该往哪个方向移动他才能到达最低点）。经过研究当沿着当前点所在位置的切线方向走是最快的，且可以到达最低点。所以就有了下面的公式。（其中<code>lr</code>为学习率，也就是防止走的过快超过了最低点）</p>
<p>\[<br/>
\begin{eqnarray*}<br/>
w^{&#39;}&amp;=&amp;w-lr*\frac{\partial{loss}}{\partial{w}} \\<br/>
\frac{\partial{loss}}{\partial{w}}&amp;=&amp;2*\sum_{i=1}^{n}{(w*x_i+b-y_i)*x_i} \\<br/>
b^{&#39;}&amp;=&amp;b-lr*\frac{\partial{loss}}{\partial{b}} \\<br/>
\frac{\partial{loss}}{\partial{b}}&amp;=&amp;2*\sum_{i=1}^{n}{(w*x_i+b-y_i)}<br/>
\end{eqnarray*}<br/>
\]</p>
<ul>
<li>梯度下降代码</li>
</ul>
<pre><code class="language-text"># b_current 当前的 b 值
# w_current 当前的 w 值<br/>
# points 数据 <br/>
# learningRate 学习率<br/>
def step_gradient(b_current, w_current, points, learningRate):<br/>
    # b 的梯度，也就是 b 导数值<br/>
    b_gradient = 0<br/>
    # w 的梯度，也就是 w 导数值<br/>
    w_gradient = 0<br/>
    # 数据长度<br/>
    N = float(len(points))<br/>
    # 上面公式中累加的过程<br/>
    for i in range(0, len(points)):<br/>
        # 取出 x 值<br/>
        x = points[i, 0]<br/>
        # 取出 y 值<br/>
        y = points[i, 1]<br/>
        # 由求导之后的公式变成的式子<br/>
        b_gradient += (2/N) * ((w_current * x + b_current) - y)<br/>
        w_gradient += (2/N) * x * ((w_current * x + b_current) - y)<br/>
    # 式子整合求出经过梯度下降的 b<br/>
    new_b = b_current - (learningRate * b_gradient)<br/>
    new_w = w_current - (learningRate * w_gradient)<br/>
    return [new_b, new_w]
</code></pre>
<ul>
<li>通过梯度下降求出来的结果</li>
</ul>
<pre><code class="language-text"># 控制梯度下降算法迭代次数
# points 数据<br/>
# starting_b 初始化b<br/>
# starting_w 初始化w<br/>
# learning_rate 学习率<br/>
# num_iterations 迭代次数<br/>
def gradient_descent_runner(points, starting_b, starting_w, learning_rate, num_iterations):<br/>
    b = starting_b<br/>
    w = starting_w<br/>
    # 迭代<br/>
    for i in range(num_iterations):<br/>
        # 没迭代一次就调用一次梯度下降算法<br/>
        b, w = step_gradient(b, w, np.array(points), learning_rate)<br/>
    return [b, w]
</code></pre>
<pre><code class="language-text"># 运行梯度下降算法
def step_gradient_run():<br/>
    # 获取数据<br/>
    points = np.genfromtxt(&quot;data.csv&quot;, delimiter=&quot;,&quot;)<br/>
    # 初始化学习率<br/>
    learning_rate = 0.0001<br/>
    # 初始化 b 和 w<br/>
    initial_b = 0<br/>
    initial_w = 0<br/>
    # 初始化迭代次数<br/>
    num_iterations = 1000<br/>
    print(&quot;b = {0}, w = {1}, error = {2}&quot;<br/>
          .format(initial_b, initial_w, compute_error_for_line_given_points(initial_b, initial_w, points)))<br/>
    [b, w] = gradient_descent_runner(points, initial_b, initial_w, learning_rate, num_iterations)<br/>
    # 画图 x 点随机生成<br/>
    x = np.linspace(1, 100, 100)<br/>
    # 根据 x w b 求出 y<br/>
    y = x * w + b<br/>
    # 画数据点<br/>
    plt.scatter(points[:, 0], points[:, 1])<br/>
    # 划线<br/>
    plt.plot(x, y)<br/>
    plt.show()<br/>
    print(&quot;b = {0}, w = {1}, error = {2}&quot;<br/>
          .format(b, w, compute_error_for_line_given_points(b, w, points)))
</code></pre>
<p><img src="media/16022106879503/16022595564125.png" alt=""/></p></li>
</ul></li>
<li><p><a href="https://github.com/ncdhz/deep-learning-study-code/tree/master/linear-regression">项目地址</a></p></li>
</ol>

                  </article>
                  <div class="comments-wrap">
                    <div class="share-comments">
                      

                      

                      
                    </div>
                  </div><!-- end comments wrap -->
              </div>
            </div><!-- end columns -->
      </div><!-- end container -->
    </section>



    <footer class="footer">
        <div class="content has-text-centered">
          <p>
              Copyright &copy; 2019
              Powered by <a target="_blank" href="http://www.mweb.im">MWeb</a>,&nbsp; 
              Theme used <a target="_blank" href="https://bulma.io/">Bulma CSS</a>.
          </p>
        </div>
      </footer>



<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({TeX: { equationNumbers: { autoNumber: "AMS" } }});</script>

  













<script src="asset/prism.js"></script>



  
    




  </body>
</html>
